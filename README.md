# ğŸ§  ML-Based Self-Identification of Mental Health

This academic mini project aims to detect and classify a personâ€™s emotional state using **voice** and **facial expressions**. By combining speech and image data, the system attempts to provide an early indication of mental health status using machine learning techniques.

---

## ğŸ“Œ Project Overview

The system is divided into three core modules:

1. **Voice Emotion Recognition** â€“ Classifies emotions based on audio signals
2. **Facial Emotion Recognition** â€“ Identifies emotions from facial images
3. **Integrated Model** â€“ Combines predictions from both modules for better accuracy

This project was developed as a team effort during our final year of B.Tech in Information Technology.

---

## ğŸ§© Modules

### ğŸ™ï¸ Voice Emotion Recognition
- **Dataset:** RAVDESS (not included in repo due to size)
- **Features Used:** MFCC, Chroma, Mel Spectrogram
- **Model:** CNN-LSTM
- **Output:** Emotion label (e.g., happy, sad, angry)

### ğŸ˜€ Facial Emotion Recognition
- **Dataset:** FER2013 (publicly available on Kaggle)
- **Model:** CNN
- **Output:** Facial emotion classification into predefined categories

### ğŸ” Integrated Model
- Merges predictions from both modules
- Gives a more reliable assessment of emotional state

---

## ğŸ› ï¸ Technologies Used

- Python
- TensorFlow / Keras
- NumPy, Pandas
- Librosa (audio feature extraction)
- OpenCV (image processing)
- Scikit-learn
- Matplotlib (visualization)

---
